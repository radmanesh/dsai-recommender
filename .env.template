# ============================================================================
# Agentic Research Matchmaker - Environment Configuration Template
# ============================================================================
# Copy this file to .env and fill in your values:
#   cp .env.template .env
# ============================================================================

# ----------------------------------------------------------------------------
# HuggingFace Configuration (REQUIRED)
# ----------------------------------------------------------------------------
# Get your token from: https://huggingface.co/settings/tokens
# Required for downloading models initially, not needed for inference after download
HF_TOKEN=your_huggingface_token_here

# ----------------------------------------------------------------------------
# Data Paths
# ----------------------------------------------------------------------------
# Directory containing faculty data
DATA_DIR=data

# Path to faculty CSV file
CSV_PATH=data/DSAI-Faculties.csv

# Directory containing faculty PDFs (CVs, papers, proposals)
PDF_DIR=data/pdfs

# ----------------------------------------------------------------------------
# ChromaDB Configuration
# ----------------------------------------------------------------------------
# Path where ChromaDB will store vector data
CHROMA_PATH=./faculty_chroma_db

# Name of the ChromaDB collection
COLLECTION_NAME=faculty_all

# ----------------------------------------------------------------------------
# Model Configuration
# ----------------------------------------------------------------------------
# Embedding model for semantic search (local, runs on CPU)
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5

# LLM model for reasoning (local inference)
# Options:
#   - Qwen/Qwen2.5-Coder-1.5B-Instruct  (Default, ~3GB, CPU-friendly)
#   - Qwen/Qwen2.5-Coder-3B-Instruct    (Medium, ~6GB, needs GPU or good CPU)
#   - Qwen/Qwen2.5-Coder-7B-Instruct    (Large, ~14GB, needs GPU with 8GB+ VRAM)
LLM_MODEL=Qwen/Qwen2.5-Coder-3B-Instruct

# ----------------------------------------------------------------------------
# LLM Parameters
# ----------------------------------------------------------------------------
# Temperature for text generation (0.0 = deterministic, 1.0 = creative)
# Lower values (0.1-0.3) recommended for consistent, factual outputs
LLM_TEMPERATURE=0.2

# Maximum number of tokens to generate per response
LLM_MAX_TOKENS=512

# Enable 8-bit quantization to reduce memory usage by ~50% (GPU only)
# Options: true, false
USE_8BIT_QUANTIZATION=false

# Device to run the model on
# Options: auto (auto-detect), cuda (GPU), cpu (CPU), mps (Apple Silicon)
LLM_DEVICE=auto

# ----------------------------------------------------------------------------
# Retrieval Configuration
# ----------------------------------------------------------------------------
# Number of top results to retrieve from vector database
TOP_K_RESULTS=10

# Size of text chunks for embedding (in characters)
# Larger chunks: better context, slower processing
# Recommended: 512-2048 for academic text
CHUNK_SIZE=1024

# Overlap between consecutive chunks (in characters)
# Ensures continuity across chunk boundaries
CHUNK_OVERLAP=128

# ============================================================================
# Notes:
# ============================================================================
# - First run will download the LLM model (~3-14GB depending on model choice)
# - Models are cached at: ~/.cache/huggingface/hub/
# - ChromaDB will be created when you run: python scripts/ingest.py
# - Run 'python scripts/select_model.py' to check your hardware capabilities
# - Run 'python scripts/test_local_model.py' to verify model loading
# ============================================================================
